source ~/.bashrc
use Oracle-full-client

#
# Cron job that runs on the Mercury app server machine as part of Mercury ETL to data warehouse.
# The script manages moving operational data from files made by Mercury Envers, to the data
# warehouse input tables, using SqlLoader.
#
# Directories
#
#   Looks in directory ${NEW_DIR} for new data files to process.
#   sqlLoader will write files to ${LOG_DIR} and ${ERROR_DIR}
#   Processed files are moved to ${DONE_DIR}, regardless of their success/fail status.
#
# File Sets
#
#   Expects to find multiple data files that are interrelated and need to be processed
#   together by the database merge, though they can all be loaded into the import tables
#   regardless of order.  Interrelated data files all have the same etl date in their filename.
#
#   Filename contains etl date and table name:
#     <YYYYMMDDHHMMSSFFF>_<table_name>.dat
#     e.g. 20121025152056211_product_order.dat
#
#   There must also be a sqlldr control file for each table name, e.g. product_order.ctl
#
# Indicator files
#
#   The file named <YYYYMMDDHHMMSSFFF>_is_ready  means the data file set is complete for
#   the given etl date and processing can start.
#
#   A single lock file is used while running to prevent a second cron job from starting
#   and possibly interfering with the file load.
#


#
# Reads the required params and assigns derived params
#

DB_PARAMS_FILE=~/dwdb_params

if [ ! -f ${DB_PARAMS_FILE} ]; then
    echo "Cannot find ${DB_PARAMS_FILE}" | tee /dev/stderr
    exit 1
fi
USERNAME=`grep -Ei '^username:' ${DB_PARAMS_FILE} | sed -e 's/^username://' | tr -d ' '`
PASSWORD=`grep -Ei '^password:' ${DB_PARAMS_FILE} | sed -e 's/^password://' | tr -d ' '`
DB_SERVER=`grep -Ei '^servername:' ${DB_PARAMS_FILE} | sed -e 's/^servername://' | tr -d ' '`
DB_PORT=`grep -Ei '^port:' ${DB_PARAMS_FILE} | sed -e 's/^port://' | tr -d ' '`
DB_SID=`grep -Ei '^sid:' ${DB_PARAMS_FILE} | sed -e 's/^sid://' | tr -d ' '`
ROOT_DIR=`grep -Ei '^rootDir:' ${DB_PARAMS_FILE} | sed -e 's/^rootDir://' | tr -d ' '`

NEW_DIR=${ROOT_DIR}/new
LOG_DIR=${ROOT_DIR}/log
ERROR_DIR=${ROOT_DIR}/error
DONE_DIR=${ROOT_DIR}/done
CONTROL_DIR=${ROOT_DIR}/control
LOCKFILE=${NEW_DIR}/.lockfile


#
# Validates existence of required dirs, files, and params
#

for fsobj in $ROOT_DIR $NEW_DIR $LOG_DIR $ERROR_DIR $DONE_DIR $CONTROL_DIR
do
  if [ ! -e $fsobj ]; then
      echo "Cannot find $fsobj" | tee /dev/stderr
      exit 1
  fi
done

if [ -z "${USERNAME}" ]; then
    echo "No username in ${DB_PARAMS_FILE}" | tee /dev/stderr
    exit 1
fi
if [ -z "${PASSWORD}" ]; then
    echo "No password in ${DB_PARAMS_FILE}" | tee /dev/stderr
    exit 1
fi
if [ -z "${DB_SERVER}" ]; then
    echo "No servername in ${DB_PARAMS_FILE}" | tee /dev/stderr
    exit 1
fi
if [ -z "${DB_PORT}" ]; then
    echo "No port in ${DB_PARAMS_FILE}" | tee /dev/stderr
    exit 1
fi
if [ -z "${DB_SID}" ]; then
    echo "No sid in ${DB_PARAMS_FILE}" | tee /dev/stderr
    exit 1
fi
if [ -z "${ROOT_DIR}" ]; then
    echo "No rootDir in ${DB_PARAMS_FILE}" | tee /dev/stderr
    exit 1
fi


#
# Mutexes the etl cron job in case one runs longer than the cron period.
#

# Creates a lock file, atomically.  Quits if lock file already exists.
(set -C; : > ${LOCKFILE}) 2> /dev/null
if [ $? != "0" ]; then
   echo "Previous ETL job is still running (${LOCKFILE} exists)" | tee /dev/stderr
   exit 1
fi
# Removes lockfile on <ctrl>C termination.
trap 'rm ${LOCKFILE}' EXIT


#
# Quits if there's no work to do
#

if [ -z "`find ${NEW_DIR}/ -name \*_is_ready`" ]; then
    exit 1
fi


#
# Processes etl data file sets one at a time, ordered oldest timestamp to newest timestamp.
#

find ${NEW_DIR}/ -name \*_is_ready | sort | sed -e 's!.*/!!;s!_is_ready!!' | while read TIMESTAMP
do

    # Deletes existing sqlplus output/log files in case this is a re-run of etl dat files.
    if [ -e ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log ]; then
	rm ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log
    fi
    if [ -e ${NEW_DIR}/${TIMESTAMP}_sqlplus_prep.start ]; then
	rm ${NEW_DIR}/${TIMESTAMP}_sqlplus_prep.start
    fi
    if [ -e ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log ]; then
	rm ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log
    fi
    if [ -e ${NEW_DIR}/${TIMESTAMP}_sqlplus.start ]; then
	rm ${NEW_DIR}/${TIMESTAMP}_sqlplus.start
    fi


    # Truncates all of the import tables before each set of import files is processed.
    #
    echo "Running sqlPlus to truncate import tables"
    (sqlplus -L -S "${USERNAME}/${PASSWORD}@\"(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=${DB_SERVER})(PORT=${DB_PORT})))(CONNECT_DATA=(SERVER=DEDICATED)(SID=${DB_SID})))\"" | grep -v ${TIMESTAMP}) <<!
    set pagesize 0 heading off feedback off verify off echo off trimspool on serveroutput on linesize 511
    spool ${NEW_DIR}/${TIMESTAMP}_sqlplus_prep.start;
    select to_char(${TIMESTAMP}) from dual;
    spool off;
    spool ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log;
    exec prep_import;
    quit;
!

    # Uses "sqlplus_prep.start" file to know if sqlplus actually ran because sqlplus return code is unreliable.
    if [ ! -e ${NEW_DIR}/${TIMESTAMP}_sqlplus_prep.start ]; then
	echo "SqlPlus prep_import failed to run." | tee /dev/stderr
	exit 1
    fi
    rm ${NEW_DIR}/${TIMESTAMP}_sqlplus_prep.start

    if [ -s ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log ]; then
	    echo "There were problems running merge_prep.  See ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log" | tee /dev/stderr
    else
	# removes the zero-length file
	rm ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log
    fi


    # Iterates on the etl dat files having TIMESTAMP.
    # The processing order of files within one data file set is not important.

    find ${NEW_DIR}/ -maxdepth 1 -name ${TIMESTAMP}\*.dat | while read DAT_FILE
    do
	echo "Processing ${DAT_FILE}"

	TABLE_NAME=`echo ${DAT_FILE} | sed -e "s!.*/${TIMESTAMP}_!!" | sed -e 's/.dat//'`

	CTL_FILE=${CONTROL_DIR}/${TABLE_NAME}.ctl
	# Control file for the load table must exist.
	if [ ! -f ${CTL_FILE} ]; then
	    echo "Cannot find ${CTL_FILE}" | tee /dev/stderr
	    exit 1
	fi

	BAD_FILE=${ERROR_DIR}/${TIMESTAMP}_${TABLE_NAME}.bad
	DSC_FILE=${ERROR_DIR}/${TIMESTAMP}_${TABLE_NAME}.dsc
	LOG_FILE=${LOG_DIR}/${TIMESTAMP}_${TABLE_NAME}.log

	echo "Running SqlLoader"
	sqlldr silent="(HEADER)" control=${CTL_FILE} log=${LOG_FILE} bad=${BAD_FILE} data=${DAT_FILE} discard=${DSC_FILE} direct=true userid="${USERNAME}/${PASSWORD}@\"(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=${DB_SERVER})(PORT=${DB_PORT})))(CONNECT_DATA=(SERVER=DEDICATED)(SID=${DB_SID})))\""

	case $? in
        1|3) echo "SqlLoader had fatal error loading ${DAT_FILE}" | tee /dev/stderr
             exit 1
	     ;;
        2)   echo "SqlLoader could not load some records. See ${ERROR_DIR}/${BAD_FILE}" | tee /dev/stderr
	     ;;
	esac

        # Moves the data file to done.
	mv -f ${DAT_FILE} ${DONE_DIR}
    done

    # If there are any remaining unprocessed dat files having TIMESTAMP, assume
    # there was a fatal error. Skips sqlplus and resets any done files back to new.

    UNPROCESSED_FILE=`find ${NEW_DIR}/ -maxdepth 1 -name ${TIMESTAMP}\*.dat | head -1`

    if [ -n "${UNPROCESSED_FILE}" ]; then
	echo "Unprocessed files were found.  Resetting processed files back to new and quitting." | tee /dev/stderr
	mv -f ${DONE_DIR}/${TIMESTAMP}*.dat ${NEW_DIR}
        exit 1
    fi

    # Invokes load_merge stored procedure and waits for it to finish.
    echo "Running sqlPlus to merge new data"
    (sqlplus -L -S "${USERNAME}/${PASSWORD}@\"(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=${DB_SERVER})(PORT=${DB_PORT})))(CONNECT_DATA=(SERVER=DEDICATED)(SID=${DB_SID})))\"" | grep -v ${TIMESTAMP}) <<!
    set pagesize 0 heading off feedback off verify off echo off trimspool on serveroutput on linesize 511
    spool ${NEW_DIR}/${TIMESTAMP}_sqlplus.start;
    select to_char(${TIMESTAMP}) from dual;
    spool off;
    spool ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log;
    exec merge_import;
    quit;
!

    if [ ! -e ${NEW_DIR}/${TIMESTAMP}_sqlplus.start ]; then
        echo "SqlPlus failed to run.  Resetting processed files back to new and quitting." | tee /dev/stderr
	mv -f ${DONE_DIR}/${TIMESTAMP}*.dat ${NEW_DIR}
        exit 1
    fi
    rm ${NEW_DIR}/${TIMESTAMP}_sqlplus.start

    if [ -s ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log ]; then
	echo "There were data problems while merging new data.  See ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log" | tee /dev/stderr
    else
        # removes the zero-length file
        rm ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log
    fi
    mv -f ${NEW_DIR}/${TIMESTAMP}_is_ready ${DONE_DIR}

done
