source ~/.bashrc
use Oracle-full-client
umask 0111

#
# Cron job that runs on the Mercury app server machine as part of Mercury ETL to data warehouse.
# The script manages moving operational data from files made by Mercury Envers, to the data
# warehouse input tables, using SqlLoader.
#
# Directories
#
#   Looks in directory ${NEW_DIR} for new data files to process.
#   sqlLoader will write files to ${LOG_DIR} and ${ERROR_DIR}
#   Processed files are moved to ${DONE_DIR}, regardless of their success/fail status.
#
# File Sets
#
#   Expects to find multiple data files that are interrelated and need to be processed
#   together by the database merge, though they can all be loaded into the import tables
#   regardless of order.  Interrelated data files all have the same etl date in their filename.
#
#   Filename contains etl date and table name:
#     <YYYYMMDDHHMMSSFFF>_<table_name>.dat
#     e.g. 20121025152056211_product_order.dat
#
#   There must also be a sqlldr control file for each table name, e.g. product_order.ctl
#
# Indicator files
#
#   The file named <YYYYMMDDHHMMSSFFF>_is_ready  means the data file set is complete for
#   the given etl date and processing can start.
#
#   A single lock file is used while running to prevent a second cron job from starting
#   and possibly interfering with the file load.
#


#
# Reads the required params and assigns derived params
#

DB_PARAMS_FILE="$1"

if [ -z "${DB_PARAMS_FILE}" ]; then
    echo "Missing db params filename parameter" >&2
    exit 1
fi
if [ ! -f ${DB_PARAMS_FILE} ]; then
    echo "Cannot find ${DB_PARAMS_FILE}" >&2
    exit 1
fi
USERNAME=`grep -Ei '^username:' ${DB_PARAMS_FILE} | sed -e 's/^username://' | tr -d ' '`
PASSWORD=`grep -Ei '^password:' ${DB_PARAMS_FILE} | sed -e 's/^password://' | tr -d ' '`
DB_SERVER=`grep -Ei '^servername:' ${DB_PARAMS_FILE} | sed -e 's/^servername://' | tr -d ' '`
DB_PORT=`grep -Ei '^port:' ${DB_PARAMS_FILE} | sed -e 's/^port://' | tr -d ' '`
DB_SID=`grep -Ei '^sid:' ${DB_PARAMS_FILE} | sed -e 's/^sid://' | tr -d ' '`
ROOT_DIR=`grep -Ei '^rootDir:' ${DB_PARAMS_FILE} | sed -e 's/^rootDir://' | tr -d ' '`

NEW_DIR=${ROOT_DIR}/new
LOG_DIR=${ROOT_DIR}/log
ERROR_DIR=${ROOT_DIR}/error
DONE_DIR=${ROOT_DIR}/done
CONTROL_DIR=${ROOT_DIR}/control
LOCKFILE=${NEW_DIR}/.lockfile


#
# Validates existence of required dirs, files, and params
#

for fsobj in $ROOT_DIR $NEW_DIR $LOG_DIR $ERROR_DIR $DONE_DIR $CONTROL_DIR
do
  if [ ! -e $fsobj ]; then
      echo "Cannot find $fsobj" >&2
      exit 1
  fi
done

if [ -z "${USERNAME}" ]; then
    echo "No username in ${DB_PARAMS_FILE}" >&2
    exit 1
fi
if [ -z "${PASSWORD}" ]; then
    echo "No password in ${DB_PARAMS_FILE}" >&2
    exit 1
fi
if [ -z "${DB_SERVER}" ]; then
    echo "No servername in ${DB_PARAMS_FILE}" >&2
    exit 1
fi
if [ -z "${DB_PORT}" ]; then
    echo "No port in ${DB_PARAMS_FILE}" >&2
    exit 1
fi
if [ -z "${DB_SID}" ]; then
    echo "No sid in ${DB_PARAMS_FILE}" >&2
    exit 1
fi
if [ -z "${ROOT_DIR}" ]; then
    echo "No rootDir in ${DB_PARAMS_FILE}" >&2
    exit 1
fi


#
# Mutexes the etl cron job in case one runs longer than the cron period.
#

# Creates a lock file, atomically.  Quits if lock file already exists.
(set -C; : > ${LOCKFILE}) 2> /dev/null
if [ $? != "0" ]; then
   echo "Previous ETL job is still running (${LOCKFILE} exists)" >&2
   exit 1
fi
# Removes lockfile on <ctrl>C termination.
trap 'rm ${LOCKFILE}' EXIT


#
# Quits if there's no work to do
#

if [ -z "`find ${NEW_DIR}/ -maxdepth 1 -name \*_is_ready`" ]; then
    exit 1
fi


#
# Processes etl data file sets one at a time, ordered oldest timestamp to newest timestamp.
#

ls -1 ${NEW_DIR}/*_is_ready | sort -u | sed -e 's!.*/!!;s!_is_ready!!' | while read TIMESTAMP
do

    # Deletes existing sqlplus output/log files in case this is a re-run of etl dat files.
    if [ -e ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log ]; then
	rm ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log
    fi
    if [ -e ${NEW_DIR}/${TIMESTAMP}_sqlplus_prep.start ]; then
	rm ${NEW_DIR}/${TIMESTAMP}_sqlplus_prep.start
    fi
    if [ -e ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log ]; then
	rm ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log
    fi
    if [ -e ${NEW_DIR}/${TIMESTAMP}_sqlplus.start ]; then
	rm ${NEW_DIR}/${TIMESTAMP}_sqlplus.start
    fi


    # Truncates all of the import tables before each set of import files is processed.
    #
    echo "Running sqlPlus to truncate import tables"
    (sqlplus -L -S "${USERNAME}/${PASSWORD}@\"(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=${DB_SERVER})(PORT=${DB_PORT})))(CONNECT_DATA=(SERVER=DEDICATED)(SID=${DB_SID})))\"" | grep -v ${TIMESTAMP}) <<!
    set pagesize 0 heading off feedback off verify off echo off trimspool on serveroutput on linesize 511
    spool ${NEW_DIR}/${TIMESTAMP}_sqlplus_prep.start;
    select to_char(${TIMESTAMP}) from dual;
    spool off;
    spool ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log;
    exec prep_import;
    quit;
!

    # Uses "sqlplus_prep.start" file to know if sqlplus actually ran because sqlplus return code is unreliable.
    if [ ! -e ${NEW_DIR}/${TIMESTAMP}_sqlplus_prep.start ]; then
	echo "SqlPlus prep_import failed to run.  Will retry in the next ETL run." >&2
	exit 1
    fi
    rm ${NEW_DIR}/${TIMESTAMP}_sqlplus_prep.start

    if [ -s ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log ]; then
            echo "Encountered errors running prep_import stored procedure.  ETL processing stops.  Will retry in the next ETL run." >&2
            echo "${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log contents:" >&2
            head -15 ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log >&2
            exit 1
    else
	# removes the zero-length file
	rm ${ERROR_DIR}/${TIMESTAMP}_sqlplus_prep.log
    fi


    # Iterates on the etl dat files having TIMESTAMP.
    # The processing order of files within one data file set is not important.

    find ${NEW_DIR}/ -maxdepth 1 -name ${TIMESTAMP}\*.dat | while read DAT_FILE
    do
	echo "Processing ${DAT_FILE}"

	TABLE_NAME=`echo ${DAT_FILE} | sed -e "s!.*/${TIMESTAMP}_!!" | sed -e 's/.dat$//'`

	CTL_FILE=${CONTROL_DIR}/${TABLE_NAME}.ctl
	# Control file for the load table must exist.
	if [ ! -f ${CTL_FILE} ]; then
	    echo "Cannot find ${CTL_FILE}" >&2
	    exit 1
	fi

	BAD_FILE=${ERROR_DIR}/${TIMESTAMP}_${TABLE_NAME}.bad
	DSC_FILE=${ERROR_DIR}/${TIMESTAMP}_${TABLE_NAME}.dsc
	LOG_FILE=${LOG_DIR}/${TIMESTAMP}_${TABLE_NAME}.log

	echo "Running SqlLoader"
	sqlldr silent="(HEADER)" control=${CTL_FILE} log=${LOG_FILE} bad=${BAD_FILE} data=${DAT_FILE} discard=${DSC_FILE} direct=true userid="${USERNAME}/${PASSWORD}@\"(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=${DB_SERVER})(PORT=${DB_PORT})))(CONNECT_DATA=(SERVER=DEDICATED)(SID=${DB_SID})))\""

	case $? in
        1|3) echo "** Manual intervention is needed **" >&2
             echo "SqlLoader had fatal error loading ${DAT_FILE}" >&2
             exit 1
	     ;;
        2)   echo "** Manual intervention is needed **" >&2
             if [ -n "`hexdump -n 500 ${BAD_FILE} | cut -c 8- | grep -m 1 -q 0d && echo Y`" ]; then
               echo "${BAD_FILE}  has DOS line termination.  Cannot process it." >&2
             else
               echo "SqlLoader could not load records in ${BAD_FILE}:" >&2
               head -10 ${BAD_FILE} >&2
               echo "Check for mismatch with control file, and import table column definition (number of fields, "
               echo "field datatype, field width, control names match import table column names)."
             fi
	     ;;
	esac

        # Moves the data file to done.
	mv -f ${DAT_FILE} ${DONE_DIR}
    done

    # If there are any remaining unprocessed dat files having TIMESTAMP there was probably some fatal error.
    # Moves the files to DONE but abandons them.  Also abandons the import table data.

    if [ -n "`find ${NEW_DIR}/ -maxdepth 1 -name ${TIMESTAMP}\*.dat`" ]; then
	echo "Unprocessed files were found:" >&2
        ls -1 ${NEW_DIR}/${TIMESTAMP}*.dat >&2
	mv ${NEW_DIR}/${TIMESTAMP}_* ${DONE_DIR}

        if [ -n "`find ${DONE_DIR}/ -maxdepth 1 -name ${TIMESTAMP}\*.dat`" ]; then
           echo "** Manual intervention is needed **" >&2
	   echo "Data from these files will neither be in the warehouse nor get picked up by a later ETL:" >&2
           ls -1 ${DONE_DIR}/${TIMESTAMP}*.dat >&2
        fi
        exit 1
    fi

    # Invokes load_merge stored procedure and waits for it to finish.
    echo "Running sqlPlus to merge new data"
    (sqlplus -L -S "${USERNAME}/${PASSWORD}@\"(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=${DB_SERVER})(PORT=${DB_PORT})))(CONNECT_DATA=(SERVER=DEDICATED)(SID=${DB_SID})))\"" | grep -v ${TIMESTAMP}) <<!
    set pagesize 0 heading off feedback off verify off echo off trimspool on serveroutput on linesize 511
    spool ${NEW_DIR}/${TIMESTAMP}_sqlplus.start;
    select to_char(${TIMESTAMP}) from dual;
    spool off;
    spool ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log;
    exec MERGE_ETL_IMPORT.DO_ETL;
    quit;
!

    if [ ! -e ${NEW_DIR}/${TIMESTAMP}_sqlplus.start ]; then
        echo "SqlPlus failed to run, but will retry automatically in the next ETL run." >&2
	mv -f ${DONE_DIR}/${TIMESTAMP}*.dat ${NEW_DIR}
        exit 1
    fi
    rm ${NEW_DIR}/${TIMESTAMP}_sqlplus.start

    if [ -s ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log ]; then
        grep -Fq "ORA-" ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log
        if [ $? -eq 0 ]; then
            echo "** Manual intervention is needed **" >&2
            echo "There were data problems running the merge stored procedure." >&2
            echo "${ERROR_DIR}/${TIMESTAMP}_sqlplus.log shows these errors:" >&2
            ~/dw_etl_parse_merge_errors.pl ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log ${DONE_DIR}/ >&2
        fi
    else
        # removes the zero-length file
        rm ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log
    fi
    mv -f ${NEW_DIR}/${TIMESTAMP}_is_ready ${DONE_DIR}

done
