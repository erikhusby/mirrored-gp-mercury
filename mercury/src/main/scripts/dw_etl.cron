source ~/.bashrc
use Oracle-full-client

#
# Cron job that runs on the Mercury app server machine as part of Mercury ETL to data warehouse.
# The script manages moving operational data from files made by Mercury Envers, to the data
# warehouse input tables, using SqlLoader.
#
# Directories
#
#   Looks in directory ${NEW_DIR} for new data files to process.
#   sqlLoader will write files to ${LOG_DIR} and ${ERROR_DIR}
#   Processed files are moved to ${DONE_DIR}, regardless of their success/fail status.
#
# File Sets
#
#   Expects to find multiple data files that are interrelated and need to be processed
#   together by the database merge, though they can all be loaded into the import tables
#   regardless of order.  Interrelated data files all have the same etl date in their filename.
#
#   Filename contains etl date and table name:
#     <YYYYMMDDHHMMSSFFF>_<table_name>.dat
#     e.g. 20121025152056211_product_order.dat
#
#   There must also be a sqlldr control file for each table name, e.g. product_order.ctl
#
# Indicator files
#
#   The file named <YYYYMMDDHHMMSSFFF>_is_ready  means the data file set is complete for
#   the given etl date and processing can start.
#
#   A single lock file is used while running to prevent a second cron job from starting
#   and possibly interfering with the file load.
#

ROOT_DIR=/seq/lims/datawh/dev
NEW_DIR=${ROOT_DIR}/new
LOG_DIR=${ROOT_DIR}/log
ERROR_DIR=${ROOT_DIR}/error
DONE_DIR=${ROOT_DIR}/done
CONTROL_DIR=${ROOT_DIR}/control
LOCKFILE=${NEW_DIR}/.lockfile
DB_PARAMS_FILE=~/dwdb_params

# Checks for required dirs and files.
for fsobj in $ROOT_DIR $NEW_DIR $LOG_DIR $ERROR_DIR $DONE_DIR $CONTROL_DIR
do
  if [ ! -e $fsobj ]; then
      echo "Cannot find $fsobj"
      exit 1
  fi
done

# Creates a lock file, atomically.  Quits if lock file already exists.
(set -C; : > ${LOCKFILE}) 2> /dev/null
if [ $? != "0" ]; then
   echo "Previous ETL job is still running (${LOCKFILE} exists)"
   exit 1
fi
# Removes lockfile when <ctrl>C termination.
trap 'rm ${LOCKFILE}' EXIT

# Reads file and validates the db parameters
if [ ! -f ${DB_PARAMS_FILE} ]; then
    echo "Cannot find ${DB_PARAMS_FILE}"
    exit 1
fi
USERNAME=`grep -Ei '^username:' ${DB_PARAMS_FILE} | sed -e 's/^username://' | tr -d ' '`
PASSWORD=`grep -Ei '^password:' ${DB_PARAMS_FILE} | sed -e 's/^password://' | tr -d ' '`
DB_SERVER=`grep -Ei '^servername:' ${DB_PARAMS_FILE} | sed -e 's/^servername://' | tr -d ' '`
DB_PORT=`grep -Ei '^port:' ${DB_PARAMS_FILE} | sed -e 's/^port://' | tr -d ' '`
DB_SID=`grep -Ei '^sid:' ${DB_PARAMS_FILE} | sed -e 's/^sid://' | tr -d ' '`

if [ -z "${USERNAME}" ]; then
    echo "No username in ${DB_PARAMS_FILE}"
    exit 1
fi
if [ -z "${PASSWORD}" ]; then
    echo "No password in ${DB_PARAMS_FILE}"
    exit 1
fi
if [ -z "${DB_SERVER}" ]; then
    echo "No servername in ${DB_PARAMS_FILE}"
    exit 1
fi
if [ -z "${DB_PORT}" ]; then
    echo "No port in ${DB_PARAMS_FILE}"
    exit 1
fi
if [ -z "${DB_SID}" ]; then
    echo "No sid in ${DB_PARAMS_FILE}"
    exit 1
fi

if [ -z "`find ${NEW_DIR}/ -name \*_is_ready`" ]; then
    echo "No data files to process."
    exit 1
fi

# Iterates on ready to run etl data file sets, oldest timestamp to newest timestamp.
find ${NEW_DIR}/ -name \*_is_ready | sort | sed -e 's!.*/!!;s!_is_ready!!' | while read TIMESTAMP
do

    # Deletes indicator files in case the same etl dat files are re-run.
    if [ -e ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log ]; then
	rm ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log
    fi
    if [ -e ${NEW_DIR}/${TIMESTAMP}_sqlplus.start ]; then
	rm ${NEW_DIR}/${TIMESTAMP}_sqlplus.start
    fi


    # Iterates on the etl dat files having TIMESTAMP
    find ${NEW_DIR}/ -maxdepth 1 -name ${TIMESTAMP}\*.dat | while read DAT_FILE
    do
        echo ' '
	echo "Processing ${DAT_FILE}"

	TABLE_NAME=`echo ${DAT_FILE} | sed -e "s!.*/${TIMESTAMP}_!!" | sed -e 's/.dat//'`

	CTL_FILE=${CONTROL_DIR}/${TABLE_NAME}.ctl
	# Control file for the load table must exist.
	if [ ! -f ${CTL_FILE} ]; then
	    echo "Cannot find ${CTL_FILE}"
	    exit 1
	fi

	BAD_FILE=${ERROR_DIR}/${TIMESTAMP}_${TABLE_NAME}.bad
	DSC_FILE=${ERROR_DIR}/${TIMESTAMP}_${TABLE_NAME}.dsc
	LOG_FILE=${LOG_DIR}/${TIMESTAMP}_${TABLE_NAME}.log

	echo "Running SqlLoader"
	sqlldr silent="(HEADER)" control=${CTL_FILE} log=${LOG_FILE} bad=${BAD_FILE} data=${DAT_FILE} discard=${DSC_FILE} direct=true userid="${USERNAME}/${PASSWORD}@\"(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=${DB_SERVER})(PORT=${DB_PORT})))(CONNECT_DATA=(SERVER=DEDICATED)(SID=${DB_SID})))\""

	case $? in
        1|3) echo ' '
             echo "SqlLoader had fatal error."
             exit 1
	     ;;
        2)   echo ' ' 
             echo "SqlLoader could not load some records. Check files  ${ERROR_DIR}/${TIMESTAMP}_*"
	     ;;
	esac

        # Moves the data file to done.
	mv ${DAT_FILE} ${DONE_DIR}
    done

    # If there are any unprocessed dat files having TIMESTAMP, then there must
    # have been a fatal error. Skip sqlplus and put any done files back to new.

    UNPROCESSED_FILE=`find ${NEW_DIR}/ -maxdepth 1 -name ${TIMESTAMP}\*.dat | head -1`

    if [ -n "${UNPROCESSED_FILE}" ]; then
	echo "Resetting processed files back to new and quitting."
	mv ${DONE_DIR}/${TIMESTAMP}*.dat ${NEW_DIR}
        exit 1
    fi

    # Invokes load_merge stored procedure and waits for it to finish.
    echo ' '
    echo "Running sqlPlus to merge new data"
    (sqlplus -L -S "${USERNAME}/${PASSWORD}@\"(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=${DB_SERVER})(PORT=${DB_PORT})))(CONNECT_DATA=(SERVER=DEDICATED)(SID=${DB_SID})))\"" | grep -v ${TIMESTAMP}) <<!
set pagesize 0 heading off feedback off verify off echo off trimspool on serveroutput on linesize 511
spool ${NEW_DIR}/${TIMESTAMP}_sqlplus.start;
select to_char(${TIMESTAMP}) from dual;
spool off;
spool ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log;
exec merge_import;
quit;
!
    if [ ! -e ${NEW_DIR}/${TIMESTAMP}_sqlplus.start ]; then
        echo "SqlPlus failed to run.  Resetting processed files back to new and quitting."
	mv ${DONE_DIR}/${TIMESTAMP}*.dat ${NEW_DIR}
        exit 1
    fi
    rm ${NEW_DIR}/${TIMESTAMP}_sqlplus.start

    if [ -s ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log ]; then
	echo "There were data problems while merging new data.  See ${ERROR_DIR}/${TIMESTAMP}_sqlplus.log"
    fi
    mv ${NEW_DIR}/${TIMESTAMP}_is_ready ${DONE_DIR}

done
